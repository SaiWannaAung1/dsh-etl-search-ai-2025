```aiignore
To implement Retrieval Augmented Generation (RAG) in your project, you need to add a "Generator" (a Large Language Model like GPT-4, Llama 3, or Gemini) to the end of your search pipeline.

The flow changes from: Search: Query → Vector → Top Chunks → List of Results To: RAG: Query → Vector → Top Chunks → Prompt for LLM → Natural Language Answer

1. Create the LLM Interface
First, define how your system will talk to an AI model.

Location: DshEtlSearch.Core/Interfaces/Infrastructure/ILlmService.cs

using DshEtlSearch.Core.Common;

namespace DshEtlSearch.Core.Interfaces.Infrastructure;

public interface ILlmService
{
    Task<Result<string>> GenerateAnswerAsync(string query, string context, CancellationToken token = default);
}
2. Implement the LLM Service (Using OpenAI Example)
You can use OpenAI, Azure, or a local model via Ollama.

Location: DshEtlSearch.Infrastructure/ExternalServices/OpenAiLlmService.cs

using OpenAI.Chat; // dotnet add package OpenAI
using DshEtlSearch.Core.Common;
using DshEtlSearch.Core.Interfaces.Infrastructure;

namespace DshEtlSearch.Infrastructure.ExternalServices;

public class OpenAiLlmService : ILlmService
{
    private readonly ChatClient _client;

    public OpenAiLlmService(string apiKey)
    {
        _client = new ChatClient(model: "gpt-4o", apiKey: apiKey);
    }

    public async Task<Result<string>> GenerateAnswerAsync(string query, string context, CancellationToken token = default)
    {
        string prompt = $@"
        You are a research assistant for the CEH Catalogue. 
        Use the following extracted document snippets to answer the user's question.
        If the answer isn't in the context, say you don't know.

        CONTEXT:
        {context}

        USER QUESTION: 
        {query}

        ANSWER:";

        ChatCompletion completion = await _client.CompleteChatAsync(prompt);
        return Result<string>.Success(completion.Content[0].Text);
    }
}
3. Update the Search Controller for RAG
We add a new endpoint /api/search/ask that combines your existing vector search with the LLM.

Location: DshEtlSearch.Api/Controllers/SearchController.cs

// Add ILlmService to your constructor
[HttpPost("ask")]
public async Task<ActionResult<string>> Ask([FromBody] SearchRequest request)
{
    // 1. Perform the existing Semantic Search
    var vectorResult = await _embeddingService.GenerateEmbeddingAsync(request.Query);
    var searchHits = await _vectorStore.SearchAsync(CollectionName, vectorResult.Value!, request.Limit);

    // 2. Filter and Combine the "Snippets" into one block of text (the Context)
    var context = string.Join("\n---\n", searchHits
        .Where(h => h.Score >= request.MinimumScore)
        .Select(h => h.TextContent));

    if (string.IsNullOrEmpty(context))
        return Ok("I couldn't find any relevant documents to answer your question.");

    // 3. Send the Context + Question to the LLM
    var answer = await _llmService.GenerateAnswerAsync(request.Query, context);

    return answer.IsSuccess ? Ok(answer.Value) : StatusCode(500, "AI failed to generate answer.");
}
4. Create the RAG Response Model
Location: DshEtlSearch.Api/Models/Responses/RagResponse.cs

namespace DshEtlSearch.Api.Models.Responses;

public class RagResponse
{
    public string Answer { get; set; } = string.Empty;
    public List<SearchResponse> Sources { get; set; } = new();
}
Why this is powerful for your project:
    Context-Aware: Instead of the user reading 5 different .docx files, the AI reads them all and summarizes the specific answer.
    No Hallucinations: Because you provide the context (the snippets extracted by your ZipExtractionService), the AI is forced to use real data rather than making things up.
    Efficiency: By sending only the First 200 Words (or top chunks), you stay within the LLM's token limits and save money on API costs.
To get this running:
    Install the OpenAI package: dotnet add package OpenAI
    Register the service in Program.cs: builder.Services.AddScoped<ILlmService>(s => new OpenAiLlmService("YOUR_API_KEY"));
Would you like me to help you implement "Citations"? This would allow the AI to tell you exactly which .pdf or .docx file it used for each part of the answer.
```