```aiignore
This task ties everything together. The ETL Orchestrator acts as a Facade, hiding the complexity of downloading, extracting, parsing, and saving from the rest of the application.

Branch: feature/etl-orchestrator

Step 1: Create the Branch
git checkout develop
git checkout -b feature/etl-orchestrator
Step 2: Define the Interface (Core Layer)
This interface defines the high-level use case: "Import a dataset from a URL."

File: backend/DshEtlSearch.Core/Interfaces/Application/IEtlOrchestrator.cs

using DshEtlSearch.Core.Common;

namespace DshEtlSearch.Core.Interfaces.Application
{
    public interface IEtlOrchestrator
    {
        /// <summary>
        /// Orchestrates the full ETL pipeline: Download -> Extract -> Parse -> Save.
        /// </summary>
        /// <param name="datasetUrl">The direct URL to the dataset zip file.</param>
        /// <returns>The GUID of the newly created dataset.</returns>
        Task<Result<Guid>> ImportDatasetAsync(string datasetUrl);
    }
}
Step 3: Implement the Orchestrator (Infrastructure Layer)
This service coordinates the IDownloader, IExtractionService, IMetadataParserFactory, and IMetadataRepository.

Design Note: I have implemented a "Smart Detection" logic to automatically figure out if the file is XML or JSON based on its extension.

File: backend/DshEtlSearch.Infrastructure/Services/EtlOrchestrator.cs

using DshEtlSearch.Core.Common;
using DshEtlSearch.Core.Common.Enums;
using DshEtlSearch.Core.Domain;
using DshEtlSearch.Core.Interfaces.Application;
using DshEtlSearch.Core.Interfaces.Infrastructure;
using DshEtlSearch.Infrastructure.FileProcessing.Parsers; // For Factory
using Microsoft.Extensions.Logging;

namespace DshEtlSearch.Infrastructure.Services
{
    public class EtlOrchestrator : IEtlOrchestrator
    {
        private readonly IDownloader _downloader;
        private readonly IExtractionService _extractor;
        private readonly MetadataParserFactory _parserFactory;
        private readonly IMetadataRepository _repository;
        private readonly ILogger<EtlOrchestrator> _logger;

        public EtlOrchestrator(
            IDownloader downloader,
            IExtractionService extractor,
            MetadataParserFactory parserFactory,
            IMetadataRepository repository,
            ILogger<EtlOrchestrator> logger)
        {
            _downloader = downloader;
            _extractor = extractor;
            _parserFactory = parserFactory;
            _repository = repository;
            _logger = logger;
        }

        public async Task<Result<Guid>> ImportDatasetAsync(string datasetUrl)
        {
            // 1. Validation
            if (string.IsNullOrWhiteSpace(datasetUrl))
                return Result<Guid>.Failure("Dataset URL is required.");

            // 2. Check if already exists (Idempotency)
            // Note: In a real app, you might parse the ID from the URL or metadata first. 
            // Here, we check strictly by Source URL or Identifier if known. 
            // (Skipped for MVP to keep flow simple, but recommended in production).

            string tempFolder = Path.Combine(Path.GetTempPath(), "DshEtl_" + Guid.NewGuid());
            
            try
            {
                _logger.LogInformation("Starting ETL for: {Url}", datasetUrl);

                // 3. Download
                var downloadResult = await _downloader.DownloadStreamAsync(datasetUrl);
                if (!downloadResult.IsSuccess) return Result<Guid>.Failure(downloadResult.Error!);

                using var zipStream = downloadResult.Value!;

                // 4. Extract
                var extractResult = await _extractor.ExtractZipAsync(zipStream, tempFolder);
                if (!extractResult.IsSuccess) return Result<Guid>.Failure(extractResult.Error!);

                var extractedFiles = extractResult.Value!;
                if (!extractedFiles.Any()) return Result<Guid>.Failure("Zip archive was empty.");

                // 5. Detect Metadata File (Strategy Selection)
                var metadataFile = FindMetadataFile(extractedFiles);
                if (metadataFile == null) return Result<Guid>.Failure("No suitable metadata file (XML/JSON) found in archive.");

                // 6. Select Parser
                var format = DetectFormat(metadataFile);
                var parser = _parserFactory.GetParser(format);

                // 7. Parse
                using var fileStream = File.OpenRead(metadataFile);
                var parseResult = await parser.ParseAsync(fileStream);
                
                if (!parseResult.IsSuccess) return Result<Guid>.Failure($"Parsing failed: {parseResult.Error}");

                var metadataRecord = parseResult.Value!;

                // 8. Create & Save Domain Entity
                // We use the URL as the Source Identifier for now
                var dataset = new Dataset(datasetUrl)
                {
                    Metadata = metadataRecord
                };

                // Add references to all extracted files as supporting docs
                foreach (var file in extractedFiles)
                {
                    var fileInfo = new FileInfo(file);
                    dataset.AddDocument(fileInfo.Name, DetermineFileType(fileInfo.Extension), fileInfo.Length);
                    // In a real app, upload 'file' to Blob Storage here and update StoragePath
                }

                await _repository.AddAsync(dataset);
                await _repository.SaveChangesAsync();

                _logger.LogInformation("Successfully imported dataset {Id}", dataset.Id);
                return Result<Guid>.Success(dataset.Id);
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "ETL Fatal Error");
                return Result<Guid>.Failure(ex.Message);
            }
            finally
            {
                // 9. Cleanup
                if (Directory.Exists(tempFolder))
                {
                    Directory.Delete(tempFolder, true);
                }
            }
        }

        // --- Helper Methods ---

        private string? FindMetadataFile(List<string> files)
        {
            // Priority: metadata.xml > *.xml > *.json
            return files.FirstOrDefault(f => Path.GetFileName(f).Equals("metadata.xml", StringComparison.OrdinalIgnoreCase))
                   ?? files.FirstOrDefault(f => f.EndsWith(".xml", StringComparison.OrdinalIgnoreCase))
                   ?? files.FirstOrDefault(f => f.EndsWith(".json", StringComparison.OrdinalIgnoreCase));
        }

        private MetadataFormat DetectFormat(string filePath)
        {
            if (filePath.EndsWith(".xml", StringComparison.OrdinalIgnoreCase)) return MetadataFormat.Iso19115Xml;
            if (filePath.EndsWith(".json", StringComparison.OrdinalIgnoreCase)) return MetadataFormat.JsonExpanded; // Defaulting to expanded for MVP
            throw new NotSupportedException("Unknown metadata format");
        }

        private FileType DetermineFileType(string extension)
        {
            return extension.ToLower() switch
            {
                ".xml" => FileType.Xml,
                ".json" => FileType.Json,
                ".pdf" => FileType.Pdf,
                ".zip" => FileType.Zip,
                ".txt" => FileType.Txt,
                _ => FileType.Unknown
            };
        }
    }
}
Step 4: Register in DI Container
We need to make sure the API knows about these new services.

File: backend/DshEtlSearch.Api/Configuration/ServiceCollectionExtensions.cs

Add these lines to your AddInfrastructureServices method:

// ... inside AddInfrastructureServices method ...

// Register Parsers & Factory
services.AddSingleton<MetadataParserFactory>();

// Register File Processing
services.AddHttpClient<IDownloader, CehDatasetDownloader>(); // Uses HttpClient Factory
services.AddScoped<IExtractionService, ZipExtractionService>();

// Register Orchestrator
services.AddScoped<IEtlOrchestrator, EtlOrchestrator>();
Step 5: Unit Tests (Mocking the Pipeline)
We will simulate a successful run where every component behaves correctly.

File: backend/DshEtlSearch.UnitTests/Core/Features/EtlOrchestratorTests.cs

using DshEtlSearch.Core.Common;
using DshEtlSearch.Core.Common.Enums;
using DshEtlSearch.Core.Domain;
using DshEtlSearch.Core.Interfaces.Application;
using DshEtlSearch.Core.Interfaces.Infrastructure;
using DshEtlSearch.Infrastructure.FileProcessing.Parsers;
using DshEtlSearch.Infrastructure.Services; // Ensure this points to where EtlOrchestrator is
using FluentAssertions;
using Microsoft.Extensions.Logging.Abstractions;
using Moq;
using Xunit;

namespace DshEtlSearch.UnitTests.Core.Features
{
    public class EtlOrchestratorTests
    {
        private readonly Mock<IDownloader> _downloaderMock;
        private readonly Mock<IExtractionService> _extractorMock;
        private readonly Mock<IMetadataRepository> _repoMock;
        private readonly MetadataParserFactory _factory; // Use real factory, it's stateless logic
        private readonly EtlOrchestrator _orchestrator;

        public EtlOrchestratorTests()
        {
            _downloaderMock = new Mock<IDownloader>();
            _extractorMock = new Mock<IExtractionService>();
            _repoMock = new Mock<IMetadataRepository>();
            _factory = new MetadataParserFactory();

            _orchestrator = new EtlOrchestrator(
                _downloaderMock.Object,
                _extractorMock.Object,
                _factory,
                _repoMock.Object,
                new NullLogger<EtlOrchestrator>()
            );
        }

        [Fact]
        public async Task ImportDatasetAsync_ShouldCompleteEtlFlow_WhenAllStepsSucceed()
        {
            // Arrange
            string url = "http://test.com/data.zip";
            
            // 1. Mock Download
            var dummyZipStream = new MemoryStream();
            _downloaderMock.Setup(d => d.DownloadStreamAsync(url))
                .ReturnsAsync(Result<Stream>.Success(dummyZipStream));

            // 2. Mock Extraction (Return a fake XML file path)
            // We create a real temp file so the Orchestrator can 'File.OpenRead' it
            var tempFile = Path.GetTempFileName() + ".xml";
            File.WriteAllText(tempFile, "<root>Valid XML</root>"); // Minimal valid XML content

            _extractorMock.Setup(e => e.ExtractZipAsync(It.IsAny<Stream>(), It.IsAny<string>()))
                .ReturnsAsync(Result<List<string>>.Success(new List<string> { tempFile }));

            // 3. Mock Parser (Since the Factory creates real parsers, we are testing Integration of Factory+Parser here implicitly, 
            //    or we could mock the IMetadataParser. However, EtlOrchestrator uses the Factory directly.
            //    To make this purely a UNIT test, we'd rely on the real parser failing on dummy XML unless we provide valid ISO XML.
            //    Let's provide valid ISO XML to make the Real Parser happy.)
            
            var validIsoXml = @"<?xml version='1.0'?><gmd:MD_Metadata xmlns:gmd='http://www.isotc211.org/2005/gmd' xmlns:gco='http://www.isotc211.org/2005/gco'><gmd:identificationInfo><gmd:title><gco:CharacterString>Success Title</gco:CharacterString></gmd:title></gmd:identificationInfo></gmd:MD_Metadata>";
            File.WriteAllText(tempFile, validIsoXml);

            // Act
            var result = await _orchestrator.ImportDatasetAsync(url);

            // Cleanup
            if (File.Exists(tempFile)) File.Delete(tempFile);

            // Assert
            result.IsSuccess.Should().BeTrue();
            result.Value.Should().NotBeEmpty();

            // Verify Repo was called to save
            _repoMock.Verify(r => r.AddAsync(It.Is<Dataset>(d => d.Metadata!.Title == "Success Title")), Times.Once);
            _repoMock.Verify(r => r.SaveChangesAsync(), Times.Once);
        }

        [Fact]
        public async Task ImportDatasetAsync_ShouldFail_WhenDownloadFails()
        {
            // Arrange
            _downloaderMock.Setup(d => d.DownloadStreamAsync(It.IsAny<string>()))
                .ReturnsAsync(Result<Stream>.Failure("404 Not Found"));

            // Act
            var result = await _orchestrator.ImportDatasetAsync("http://bad-url.com");

            // Assert
            result.IsSuccess.Should().BeFalse();
            result.Error.Should().Contain("404 Not Found");
            _repoMock.Verify(r => r.AddAsync(It.IsAny<Dataset>()), Times.Never);
        }
    }
}
Step 6: Execute Tests
Run the tests to confirm the flow logic.

dotnet test ./backend/DshEtlSearch.UnitTests/DshEtlSearch.UnitTests.csproj
If successful, you have now completed the entire Producer Side of the application (Downloading -> parsing -> Database). We are ready to move to Sprint 2 (Search & AI).
```